---
title: "homework"
author: "Zuo Wendi"
date: "2020-12-09"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{20071's homework}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## example1

此例子随机生成两组数据x，y，并绘制关于x,y的图像，通过修改参数来改变图形
```{r}
x=rnorm(10)
y=rnorm(10)
plot(x,y,xlab="Ten random values",ylab = "Ten oother valuse",xlim = c(-2,2),ylim = c(-2,2),pch=22,col="red",bg="green",bty="l",tcl=0.4,main = "example 1:figure",las=1,cex=1.5)
```

## example2
此例子用来展示表格
```{r}
ctl=c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
trt=c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
group=gl(2,10,20,labels = c("Ctl","Trt"))
weight=c(ctl,trt)
lm.D9=lm(weight~group)
summary(lm.D9)$coef
```
The $R^2$ is `r summary(lm.D9)$r.squared`

## example3
此例子用来展示一些公式

(1)
$\alpha+\beta$

(2)
$$\bar{X}=\frac{1}{n}\sum_{i=1}^nX_i$$
(3)
\begin{align*}
      a_1x_1+b_1x_2=c_1\\
      a_2x_1+b_2x_2=c_2
\end{align*}

(4)
$$
H(Y \mid X)=\sum_{x \in \mathcal{X}, y \in \mathcal{Y}} p(x, y) \log \left(\frac{p(x)}{p(x, y)}\right)
$$

## exercise 3.3
```{r}
n=1000
u=runif(n)   
x=2/sqrt(1-u)  #F(x)=1-(2/x)^2 ~~Pareto(2,2)
hist(x,prob=TRUE,main = expression(f(x)==16/x^3))
y=seq(0,100,.001)
lines(y,16/y^3)
```

## exercise 3.9
```{r}
n=1000
x1=runif(n,-1,1)
x2=runif(n,-1,1)
x3=runif(n,-1,1)
i=0
y=numeric(n)
while(i<n)
{
  i=i+1
  if(abs(x3[i])>=abs(x2[i]) && abs(x3[i])>=abs(x1[i]))
    {
    y[i]=x2[i]
    }
  else
    {
      y[i]=x3[i]
    }
}
hist(y,prob=TRUE,main = expression(f(x)==3/4*(1-x^2)))
z=seq(-1,0.998,.002)
lines(z,3/4*(1-z^2))
```

## exercise 3.10
proof: 

iid $x_1,x_2,x_3$~U(-1,1) 

$f(x_i)=\frac{1}{2}$   

$f(x_1,x_2,x_3)=\frac{1}{8}$  

if $\left|x_3\right|\geq\left|x_2\right|$ and $\left|x_3\right|\geq\left|x_1\right|$   

$f(x_2)=\iint_{\left|x_3\right|\geq\left|x_2\right|}^{\left|x_3\right|\geq\left|x_1\right|}\frac{1}{8}\,dx_1\,dx_3=\frac{1}{4}(1-x_2^2)$  

because $x_1,x_2,x_3$ have the same position   

$f(x_2)=3*\frac{1}{4}(1-x_2^2)=\frac{3}{4}(1-x_2^2)$   

for other conditions

$f(x_3)=\iint_{\left|x_3\right|\le\left|x_2\right|}^{\left|x_3\right|\le\left|x_1\right|}\frac{1}{8}\,dx_1\,dx_2=\frac{1}{4}(1-x_3^2)$

because $x_1,x_2,x_3$ have the same position   

$f(x_3)=3*\frac{1}{4}(1-x_3^2)=\frac{3}{4}(1-x_3^2)$  

## exercise 3.13
```{r}
n=1000
u=runif(n)   
x=2/(1-u)^(1/4)-2  #F(x)=1-(2/(2+x))^4 ~~Pareto(2,2)
hist(x,prob=TRUE,main = expression(f(x)==64/(2+x)^5))
y=seq(0,12,.001)
lines(y,64/(2+y)^5) 
```

## Question 5.1

Compute a Monte Carlo estimate of

${\int_{0}^{\pi/3}sint\,dt}$

and compare your estimate with the exact value of the integral.

## Answer 5.1

```{r}
m <- 1e4; x <- runif(m, min=0, max=pi/3)
theta.hat <- mean(sin(x)) * pi/3
print(c(theta.hat,cos(0) - cos(pi/3)))
```

## Question 5.7

Refer to Exercise 5.6. Use a Monte Carlo simulation to estimate $\theta$ by the
antithetic variate approach and by the simple Monte Carlo method. Compute
an empirical estimate of the percent reduction in variance using the antithetic
variate. Compare the result with the theoretical value from Exercise 5.6.

$\int_{0}^{1}e^x\,dx$
                  
## Answer 5.7
The simple Monte Carlo method ,the antithetic variate approach,and the theoretical value are below:
```{r}
n <- 1e4; x <- runif(n, min=0, max=1)
MC1 <- mean(exp(x))
u <- runif(n/2, min=0, max=1)
MC2 <- (sum(exp(u)+exp(1-u))/n)
print(c(MC1,MC2,exp(1)-exp(0)))

```
Their variances and the percent reduction are below:
```{r}
n <- 1e4; x <- runif(n, min=0, max=1)
v1 <- var(exp(x))/n
u <- runif(n/2, min=0, max=1)
v2 <- (var(exp(u))*(n/2-1)+var(exp(1-u))*(n/2-1))/n/(n-1)
print(c(v1,v2,v2/v1))
```

## Question 5.11

If $\hat{\theta}_1$ and $\hat{\theta}_2$ are unbiased estimators of $\theta$, and $\hat{\theta}_1$ and $\hat{\theta}_2$ are antithetic, we derived that $c^*$ = 1/2 is the optimal constant that minimizes the variance of$\hat{\theta}_c$=c$\hat{\theta}_1$ + (1 − c)$\hat{\theta}_2$. Derive $c^*$ for the general case.That is, if $\hat{\theta}_1$ and $\hat{\theta}_2$ are any two unbiased estimators of $\theta$, find the value $c^*$ that minimizes the
variance of the estimator $\hat{\theta}_c$=c$\hat{\theta}_1$ + (1 − c)$\hat{\theta}_2$ in equation (5.11). ($c^*$ will be
a function of the variances and the covariance of the estimators.)

## Answer 5.11

When $\hat{\theta}_1$ and $\hat{\theta}_2$ are unbiased estimators of $\theta$, and $\hat{\theta}_1$ and $\hat{\theta}_2$ are antithetic,the variance of$\hat{\theta}_c$=c$\hat{\theta}_1$ + (1 − c)$\hat{\theta}_2$ is :
 
 Var($\hat{\theta}_c$)
 
 = Var(c$\hat{\theta}_1$ + (1 − c)$\hat{\theta}_2$) 
 
 =$c^2$Var($\hat{\theta}_1$)+$(1-c)^2$Var($\hat{\theta}_2$)+2c(1-c)cov($\hat{\theta}_1$,$\hat{\theta}_2$)
                      
because $\hat{\theta}_1$ and $\hat{\theta}_2$ are antithetic,cov($\hat{\theta}_1$,$\hat{\theta}_2$)=0,Var($\hat{\theta}_1$)=Var($\hat{\theta}_2$),then

Var($\hat{\theta}_c$)

=Var(c$\hat{\theta}_1$ + (1 − c)$\hat{\theta}_2$)

=Var($\hat{\theta}_1$)($c^2$+$(1-c)^2$)
  
=Var($\hat{\theta}_1$)($2c^2-2c+1$)

When c=1/2, Var($\hat{\theta}_c$) takes the minimum value.

And for the general case,that is $\hat{\theta}_1$ and $\hat{\theta}_2$ are any unbiased estimators of $\theta$,the variance of$\hat{\theta}_c$=c$\hat{\theta}_1$ + (1 − c)$\hat{\theta}_2$ is :
 
 Var($\hat{\theta}_c$)
 
 = Var(c$\hat{\theta}_1$ + (1 − c)$\hat{\theta}_2$) 
 
 =$(Var(\hat{\theta}_1)+Var(\hat{\theta}_2)-2cov(\hat{\theta}_1.\hat{\theta}_2))c^2+2(cov(\hat{\theta}_1.\hat{\theta}_2)-Var(\hat{\theta}_2))c+Var(\hat{\theta}_2)$
 
When $c=\frac{Var(\hat{\theta}_2)-cov(\hat{\theta}_1.\hat{\theta}_2)}{Var(\hat{\theta}_1)+Var(\hat{\theta}_2)-2cov(\hat{\theta}_1.\hat{\theta}_2)}$, Var($\hat{\theta}_c$) takes the minimum value.

## Question 5.13

Find two importance functions f1 and f2 that are supported on (1,∞) and
are ‘close’ to

$g(x)=\frac{x^2}{\sqrt{2\pi}}e^\frac{-x^2}{2},x>1$

Which of your two importance functions should produce the smaller variance
in estimating

$\int_{1}^{\infty}\frac{x^2}{\sqrt{2\pi}}e^\frac{-x^2}{2}dx$

by importance sampling? Explain.

## Answer 5.13

I choose $f_1(x)=\frac{1}{x^2}$ and $f_2(x)=e^{1-x}$
```{r}
x=seq(1,10,.1)
plot(x,x^(-2),type="l",col="red")
par(new=TRUE)
plot(x,exp(1-x),type="l",col="blue")
lines(x,x^2*exp(-x^2/2)/sqrt(2*pi))
```

from the picture,we know that f1(x) is closer to g(x),so we predict that it has smaller variance.

```{r}
n=1e4
u=runif(n)
x1=u^(-1/2)  ##生成随机分布f1(x)
x2=1-log(u)  ##生成随机分布f2(x)
g=function(x)x^2*exp(-x^2/2)/sqrt(2*pi)  ##定义函数g(x)
f1=function(x)1/x^2    ##定义函数f1(x)
f2=function(x)exp(1-x)    ##定义函数f2(x)
theta_hat1=mean(g(x1)/f1(x1))
theta_hat2=mean(g(x2)/f2(x2))
v1=var(g(x1)/f1(x1))
v2=var(g(x2)/f2(x2))
print(c(v1,v2),2)
```
from the test above,we know that our prediction is right.

## Question 5.15

Obtain the stratified importance sampling estimate in Example 5.13 and compare
it with the result of Example 5.10.

## Answer 5.15

the stratified importance sampling estimate in Example 5.13:
```{r}
M=1e4
k=5 
r=M/k 
N=50 
T2=numeric(k)
est=matrix(0, N, 2)
g<-function(x)exp(-x)/(1+x^2)*(x>0)*(x<1)
f <- function(x)(exp(-x) / (1 - exp(-1)))
fg<-function(x)exp(-x)/(1+x^2)*(x>0)*(x<1)/(exp(-x) / (1 - exp(-1)))
for (i in 1:N) {
est[i, 1]=mean(g(runif(M)))
for(j in 1:k){
  u <- runif(M) 
  x <- - log(1 - u * (1 - exp(-1)))
  #fg <- g(x) / (exp(-x) / (1 - exp(-1)))
  T2[j]<-mean(fg(runif(M/k,- log(1 - (j-1)/k * (1 - exp(-1))),- log(1 - j/k * (1 - exp(-1))))))
}
est[i, 2]= mean(T2)
}
apply(est,2,mean)
```
the result of Example 5.10 is 
```{r}
m <- 1000
g <- function(x) {
exp(-x - log(1+x^2)) * (x > 0) * (x < 1)
}
u <- runif(m) 
x <- - log(1 - u * (1 - exp(-1)))
fg <- g(x) / (exp(-x) / (1 - exp(-1)))
theta.hat <- mean(fg)
se <- sd(fg)
print(c(theta.hat,se),2)
```

the result of the example 5.13 is closer to the true result 0.52,so the method is more useful.

## Question 6.4

Suppose that X1, . . . , Xn are a random sample from a from a lognormal distribution with unknown parameters. Construct a 95% confidence interval for
the parameter μ. Use a Monte Carlo method to obtain an empirical estimate
of the confidence level.

## Answer 6.4
since x ~ lognormal distribution ，then y = log(x) ~ normal distribution.

And the confidence interval for the parameter μ is $(\bar{log(x)}-t_{1-\frac{\alpha}{2}}(n-1)*s(log(x))/\sqrt{n},\bar{log(x)}+t_{1-\frac{\alpha}{2}}(n-1)*s(log(x))/\sqrt{n})$

```{r}
n <- 20
alpha <- .05
#x <- rlnorm(n, mean=0, sd=1)   ##   x服从对数正态分布  
#y <- log(x)   ##   y服从标准正太分布
UCL <- replicate(1000, expr = {                #上界
  y <- log(x<-rlnorm(n, mean=0, sd=1))   #生成对数正态分布
  mean(y) + qt(1-alpha/2, df=n-1) * sd(y) / sqrt(n)
} )
DCL <- replicate(1000, expr = {                #下界
  y <- log(x<-rlnorm(n, mean=0, sd=1))
  mean(y) - qt(1-alpha/2, df=n-1) * sd(y) / sqrt(n)
} )
mean(DCL<0&0<UCL)   #用蒙特卡洛估计
```

## Question 6.5

Suppose a 95% symmetric t-interval is applied to estimate a mean, but the
sample data are non-normal. Then the probability that the confidence interval
covers the mean is not necessarily equal to 0.95. Use aMonte Carlo experiment
to estimate the coverage probability of the t-interval for random samples of
χ2(2) data with sample size n = 20. Compare your t-interval results with the
simulation results in Example 6.4. (The t-interval should be more robust to
departures from normality than the interval for variance.)

## Answer 6.5

```{r}
n <- 20
alpha <- .05
UCL <- replicate(1000, expr = {                #上界
  x <- rchisq(n, df = 2)   #生成卡方正态分布
  mean(x) + qt(1-alpha/2, df=n-1) * sd(x) / sqrt(n)
} )
DCL <- replicate(1000, expr = {                #下界
  x <- rchisq(n, df = 2)   #生成卡方正态分布
  mean(x) - qt(1-alpha/2, df=n-1) * sd(x) / sqrt(n)
} )
mean(DCL<2&2<UCL)   #用蒙特卡洛估计
```
the t-interval result is more smaller than $1-\alpha=0.95$

## Question 6.7

Estimate the power of the skewness test of normality against symmetric
Beta(α, α) distributions and comment on the results. Are the results different
for heavy-tailed symmetric alternatives such as t(ν)?

## Answer 6.7 

```{r}
alpha <- 0.05
n <- 30
m <- 2500
sk <- function(x) {
  #computes the sample skewness coeff.
  xbar <- mean(x)
  m3 <- mean((x - xbar)^3)
  m2 <- mean((x - xbar)^2)
  return( m3 / m2^1.5 )
}
cv <- qnorm(1-alpha/2, 0, sqrt(6*(n-2) / ((n+1)*(n+3))))
sktests <- numeric(m)
for (i in 1:m) { 
  x <- rbeta(n, 1, 1)
  sktests[i] <- as.integer(abs(sk(x)) >= cv)
  }
pwr <- mean(sktests)
pwr
```
the result is a little smaller than alpha=0.05,we reject the H0

```{r}
alpha <- 0.05
n <- 30
m <- 2500
sk <- function(x) {
  #computes the sample skewness coeff.
  xbar <- mean(x)
  m3 <- mean((x - xbar)^3)
  m2 <- mean((x - xbar)^2)
  return( m3 / m2^1.5 )
}
cv <- qnorm(1-alpha/2, 0, sqrt(6*(n-2) / ((n+1)*(n+3))))
sktests <- numeric(m)
for (i in 1:m) { 
  x <- rt(n, 1)
  sktests[i] <- as.integer(abs(sk(x)) >= cv)
  }
pwr <- mean(sktests)
pwr
```
the result is a much bigger than alpha=0.05,we accept the H0

## Question 6.8

Refer to Example 6.16. Repeat the simulation, but also compute the F test
of equal variance, at significance level ˆα
=. 0.055. Compare the power of the
Count Five test and F test for small, medium, and large sample sizes. (Recall
that the F test is not applicable for non-normal distributions.)

## Answer 6.8

```{r}
alpha <- 0.055
n <- c(20, 100, 500)
m=1000
sigma1 <- 1
sigma2 <- 1.5
count5test <- function(x, y) {
  X <- x - mean(x)
  Y <- y - mean(y)
  outx <- sum(X > max(Y)) + sum(X < min(Y))
  outy <- sum(Y > max(X)) + sum(Y < min(X))
  # return 1 (reject) or 0 (do not reject H0)
  return(as.integer(max(c(outx, outy)) > 5))
}
test1 <- test2 <- numeric(m)
pwr1 <- pwr2 <- numeric(length(n))
for (j in 1:length(n)){
  for (i in 1:m) { 
    x <- rnorm(n[j], 0, sigma1)
    y <- rnorm(n[j], 0, sigma2)
    test1[i] <- count5test(x, y)
    test2[i] <- as.integer((var(x)/var(y))<= qf(alpha/2,n[j]-1,n[j]-1) || 
                             (var(x)/var(y))>= qf(1-alpha/2,n[j]-1,n[j]-1))
  }
  pwr1[j] <- mean(test1)
  pwr2[j] <- mean(test2)
}
print(pwr1)
print(pwr2)
```

when the samples become larger, the power becomes larger.And the power of Count Five test is smaller than that of F test

## Question 6.C

Repeat Examples 6.8 and 6.10 for Mardia’s multivariate skewness test. Mardia
[187] proposed tests of multivariate normality based on multivariate generalizations of skewness and kurtosis. If X and Y are iid, the multivariate
population skewness $\beta_{1,d}$ is defined by Mardia as $$\beta_{1,d}=E[(X-\mu)^T(\sum)^{-1}(Y-\mu)]^3$$Under normality,$\beta_{1,d}$ = 0. The multivariate skewness statistic is$$b_{1,d}=\frac{1}{n^2}\sum^{n}_{i,j=1}((X_i-\hat{X})^T(\sum)^{-1}(X_j-\hat{X}))^3$$where $\hat{\sum}$ is the maximum likelihood estimator of covariance. Large values of $b_{1,d}$ are significant. The asymptotic distribution of $nb_{1,d}/6$ is chisquared with d(d + 1)(d + 2)/6 degrees of freedom.

## Answer 6.C

## Question Discussion

If we obtain the powers for two methods under a particular simulation setting with 10,000 experiments: say, 0.651 for one method and 0.676 for another method. Can we say the powers are different at 0.05 level?

(1)What is the corresponding hypothesis test problem?

(2)What test should we use? Z-test, two-sample t-test, paired-t test or McNemar test?

(3)What information is needed to test your hypothesis?

## Answer Discussion

we can't say the powers are different at 0.05 level because the difference between 0.651 and 0.676 is not big enough

(1)H0: the powers are same    H1:the powers are different

(2)we can't use z-test because we don't know the variance
   
   we can't use two-sample t-test because two-sample t-test requires the samples are independent
   
   paired-t test or McNemar test can be used

(3)we should have the assumption of the distribution and parameters

## Question 7.1

Compute a jackknife estimate of the bias and the standard error of the correlation statistic in Example 7.2.

## Answer 7.1

```{r}
library(bootstrap)
n <- nrow(law)
R.jack <- numeric(n)
LSAT <- law$LSAT
GPA <- law$GPA*100
R.hat <- cor(LSAT,GPA)
for (i in 1:n)
  R.jack[i] <- cor(LSAT[-i],GPA[-i])
bias <- (n - 1) * (mean(R.jack - R.hat))  #jackknife estimate of bias
se <- sqrt((n-1) *(mean((R.jack - mean(R.jack))^2)))  #jackknife estimate of var
print(c(bias,se)) 
```

## Question 7.5

Refer to Exercise 7.4. Compute 95% bootstrap confidence intervals for the
mean time between failures 1/λ by the standard normal, basic, percentile,
and BCa methods. Compare the intervals and explain why they may differ.

## Answer 7.5
the MLE of $\lambda$ is $\frac{1}{\bar{x}}$

```{r}
library(boot) #for boot and boot.ci
data(aircondit, package = "boot")
theta.boot <- function(dat,i) 1/mean(dat[i])
dat <- aircondit$hours
x <- sample(dat,replace=TRUE)
boot.obj <- boot(data = x, statistic = theta.boot, R = 2000)
print(boot.ci(boot.obj,type = c("basic", "norm", "perc","bca")))
```

## Question 7.8

Refer to Exercise 7.7. Obtain the jackknife estimates of bias and standard
error of $\hat{\theta}$.

## Answer 7.8

```{r}
library(bootstrap)
x <- matrix(c(scor$mec,scor$vec,scor$alg,scor$ana,scor$sta),ncol = 5,nrow = 88)
a <- matrix(ncol = 5,nrow = 5) #计算协方差矩阵
for (i in 1:5){
  for (j in 1:5){
    a[i,j] <- cor(x[,i],x[,j])
  }
}
ev_hat <- eigen(a)
theta_hat <- ev_hat$values[1]/sum(ev_hat$values)
n <-88
theta_jack <- ev <- numeric(n)
for (m in 1:n){
  c <- matrix(ncol = 5,nrow = 5)
  y <- x[-m,]
  for (i in 1:5){
    for (j in 1:5){
      c[i,j] <- cor(y[,i],y[,j])
    }
  }
  ev <- eigen(c)
  theta_jack[m] <- ev$values[1]/sum(ev$values)
}
bias <- (n - 1) * (mean(theta_jack - theta_hat))  #jackknife estimate of bias
se <- sqrt((n-1) *(mean((theta_jack - mean(theta_jack))^2)))  #jackknife estimate of var
print(c(bias,se))
```

## Question 7.11

In Example 7.18, leave-one-out (n-fold) cross validation was used to select the
best fitting model. Use leave-two-out cross validation to compare the models.

## Answer 7.11

```{r}
library(lattice)
library(DAAG)
attach(ironslag) 
n <- length(magnetic) 
e1 <- e2 <- e3 <- e4 <- numeric(n)
for (k in 1:(n-1)) {
  y <- magnetic[c(-k,-(k+1))]
  x <- chemical[c(-k,-(k+1))]
  J1 <- lm(y ~ x)
  yhat1 <- J1$coef[1] + J1$coef[2] * chemical[k]
  e1[k] <- ((magnetic[k] - yhat1)+(magnetic[k+1] - yhat1))/2
  J2 <- lm(y ~ x + I(x^2))
  yhat2 <- J2$coef[1] + J2$coef[2] * chemical[k] +
    J2$coef[3] * chemical[k]^2
  e2[k] <- ((magnetic[k] - yhat2)+(magnetic[k+1] - yhat2))/2
  J3 <- lm(log(y) ~ x)
  logyhat3 <- J3$coef[1] + J3$coef[2] * chemical[k]
  yhat3 <- exp(logyhat3)
  e3[k] <- ((magnetic[k] - yhat3)+(magnetic[k+1] - yhat3))/2
  J4 <- lm(log(y) ~ log(x))
  logyhat4 <- J4$coef[1] + J4$coef[2] * log(chemical[k])
  yhat4 <- exp(logyhat4)
  e4[k] <- ((magnetic[k] - yhat4)+(magnetic[k+1] - yhat4))/2
}
c(mean(e1^2), mean(e2^2), mean(e3^2), mean(e4^2))
```

Using leave-two-out cross validation,the second model has the minimum MSE and is the best model.The consequences are the same with leave-one-out cross validation

## Question 8.3

The Count 5 test for equal variances in Section 6.4 is based on the maximum
number of extreme points. Example 6.15 shows that the Count 5 criterion
is not applicable for unequal sample sizes. Implement a permutation test for
equal variance based on the maximum number of extreme points that applies
when sample sizes are not necessarily equal.

## Answer 8.3

```{r}
x <- rnorm(10,0,1)
y <- rnorm(20,0,1)
z <- c(x,y)
K <- 1:30
R <- 999
conttest <- function(x,y){    #计算端点个数
  X <- x - mean(x)
  Y <- y - mean(y)
  outx <- sum(X > max(Y)) + sum(X < min(Y))
  outy <- sum(Y > max(X)) + sum(Y < min(X))
  return(max(outx,outy))
}
n0 <- conttest(x,y)   #计算原数据端点个数
n <- numeric(R)
for (i in 1:R){       #进行置换检验
  k <- sample(K,size = 10,replace = FALSE)
  x1 <- z[k]
  y1 <- z[-k]
  n[i] <- conttest(x1,y1)
}
p <- mean(c(n0,n) >= n0)    #求p
print(p)
```
because p is big, we accept H0

## Question 

Design experiments for evaluating the performance of the NN,energy, and ball methods in various situations.

(1)Unequal variances and equal expectations

(2)Unequal variances and unequal expectations

(3)Non-normal distributions: t distribution with 1 df (heavy-tailed
distribution), bimodel distribution (mixture of two normal
distributions)

(4)Unbalanced samples (say, 1 case versus 10 controls)

Note: The parameters should be chosen such that the powers
are distinguishable (say, range from 0.3 to 0.8).

## Answer 

(1)Unequal variances and equal expectations

```{r}
library(RANN) 
library(boot)
set.seed(12345)
x <- rnorm(10,0.5,0.3)
y <- rnorm(10,0.5,0.8)
N <- c(length(x), length(y))
z <- c(x, y)
Tn <- function(z, ix, sizes,k) {
  n1 <- sizes[1]; n2 <- sizes[2]; n <- n1 + n2
  if(is.vector(z)) z <- data.frame(z,0);
  z <- z[ix, ];
  NN <- nn2(data=z, k=k+1) 
  block1 <- NN$nn.idx[1:n1,-1]
  block2 <- NN$nn.idx[(n1+1):n,-1]
  i1 <- sum(block1 < n1 + .5); i2 <- sum(block2 > n1+.5)
  (i1 + i2) / (k * n)
}
boot.obj <- boot(data = z, statistic = Tn, R = 999,
                 sim = "permutation", sizes = N,k=3)
ts <- c(boot.obj$t0,boot.obj$t)
pNN.value <- mean(ts>=ts[1])
library(energy)
boot.obs <- eqdist.etest(z, sizes=N, R=999)
pe.value <- boot.obs$p.value
library(Ball)
pball.value = bd.test(x = x, y = y, R=999)$p.value
print(c(pNN.value=pNN.value,pe.value=pe.value,pball.value))
```

(2)Unequal variances and unequal expectations

```{r}
library(RANN) 
library(boot)
set.seed(12345)
x <- rnorm(10,0.5,0.3)
y <- rnorm(10,0.7,0.8)
N <- c(length(x), length(y))
z <- c(x, y)
Tn <- function(z, ix, sizes,k) {
  n1 <- sizes[1]; n2 <- sizes[2]; n <- n1 + n2
  if(is.vector(z)) z <- data.frame(z,0);
  z <- z[ix, ];
  NN <- nn2(data=z, k=k+1) 
  block1 <- NN$nn.idx[1:n1,-1]
  block2 <- NN$nn.idx[(n1+1):n,-1]
  i1 <- sum(block1 < n1 + .5); i2 <- sum(block2 > n1+.5)
  (i1 + i2) / (k * n)
}
boot.obj <- boot(data = z, statistic = Tn, R = 999,
                 sim = "permutation", sizes = N,k=3)
ts <- c(boot.obj$t0,boot.obj$t)
pNN.value <- mean(ts>=ts[1])
library(energy)
boot.obs <- eqdist.etest(z, sizes=N, R=999)
pe.value <- boot.obs$p.value
library(Ball)
pball.value = bd.test(x = x, y = y, R=999)$p.value
print(c(pNN.value=pNN.value,pe.value=pe.value,pball.value))
```

(3)Non-normal distributions: t distribution with 1 df (heavy-tailed
distribution), bimodel distribution (mixture of two normal
distributions)

```{r}
library(RANN) 
library(boot)
set.seed(12345)
x <- rt(10,1)
y1 <- rnorm(10,0.7,0.8);y2<- rnorm(10,0.4,0.3)
y=y1+y2
N <- c(length(x), length(y))
z <- c(x, y)
Tn <- function(z, ix, sizes,k) {
  n1 <- sizes[1]; n2 <- sizes[2]; n <- n1 + n2
  if(is.vector(z)) z <- data.frame(z,0);
  z <- z[ix, ];
  NN <- nn2(data=z, k=k+1) 
  block1 <- NN$nn.idx[1:n1,-1]
  block2 <- NN$nn.idx[(n1+1):n,-1]
  i1 <- sum(block1 < n1 + .5); i2 <- sum(block2 > n1+.5)
  (i1 + i2) / (k * n)
}
boot.obj <- boot(data = z, statistic = Tn, R = 999,
                 sim = "permutation", sizes = N,k=3)
ts <- c(boot.obj$t0,boot.obj$t)
pNN.value <- mean(ts>=ts[1])
library(energy)
boot.obs <- eqdist.etest(z, sizes=N, R=999)
pe.value <- boot.obs$p.value
library(Ball)
pball.value = bd.test(x = x, y = y, R=999)$p.value
print(c(pNN.value=pNN.value,pe.value=pe.value,pball.value))
```

(4)Unbalanced samples (say, 1 case versus 10 controls)

```{r}
library(RANN) 
library(boot)
set.seed(12345)
x <- rt(10,1)
y1 <- rnorm(20,0.7,0.8);y2<- rnorm(20,0.4,0.3)
y=y1+y2
N <- c(length(x), length(y))
z <- c(x, y)
Tn <- function(z, ix, sizes,k) {
  n1 <- sizes[1]; n2 <- sizes[2]; n <- n1 + n2
  if(is.vector(z)) z <- data.frame(z,0);
  z <- z[ix, ];
  NN <- nn2(data=z, k=k+1) 
  block1 <- NN$nn.idx[1:n1,-1]
  block2 <- NN$nn.idx[(n1+1):n,-1]
  i1 <- sum(block1 < n1 + .5); i2 <- sum(block2 > n1+.5)
  (i1 + i2) / (k * n)
}
boot.obj <- boot(data = z, statistic = Tn, R = 999,
                 sim = "permutation", sizes = N,k=3)
ts <- c(boot.obj$t0,boot.obj$t)
pNN.value <- mean(ts>=ts[1])
library(energy)
boot.obs <- eqdist.etest(z, sizes=N, R=999)
pe.value <- boot.obs$p.value
library(Ball)
pball.value = bd.test(x = x, y = y, R=999)$p.value
print(c(pNN.value=pNN.value,pe.value=pe.value,pball.value))
```

## Question 1

Implement a random walk Metropolis sampler for generating the standard
Laplace distribution (see Exercise 3.2). For the increment, simulate from a
normal distribution. Compare the chains generated when different variances
are used for the proposal distribution. Also, compute the acceptance rates of
each chain.

## Answer 1

```{r}
set.seed(3000)

lap_f = function(x) exp(-abs(x))

rw.Metropolis = function(sigma, x0, N){
 x = numeric(N)
 x[1] = x0
 u = runif(N)
 k = 0
 for (i in 2:N) {
  y = rnorm(1, x[i-1], sigma)
  if (u[i] <= (lap_f(y) / lap_f(x[i-1]))) x[i] = y 
  else {
  x[i] = x[i-1]
  k = k+1
  }
 }
 return(list(x = x, k = k))
}

N = 2000
sigma = c(.05, .5, 2, 16)
x0 = 25
rw1 = rw.Metropolis(sigma[1],x0,N)
rw2 = rw.Metropolis(sigma[2],x0,N)
rw3 = rw.Metropolis(sigma[3],x0,N)
rw4 = rw.Metropolis(sigma[4],x0,N)
#number of candidate points rejected
Rej = cbind(rw1$k, rw2$k, rw3$k, rw4$k)
Acc = round((N-Rej)/N,4)
rownames(Acc) = "Accept rates"
colnames(Acc) = paste("sigma",sigma)
knitr::kable(Acc)
```

```{r}
#plot  
rw = cbind(rw1$x, rw2$x, rw3$x,  rw4$x)
for (j in 1:4) {
    plot(rw[,j], type="l",
          xlab=bquote(sigma == .(round(sigma[j],3))),
          ylab="X", ylim=range(rw[,j]))
}
```
从图中可以看到，第三幅图的收敛性最好。

## Question 2

For Exercise 9.4, use the Gelman-Rubin method to monitor convergence of the chain, and run the chain until it converges approximately to the target distribution according to $\hat{R}$ < 1.2.

## Answer 2

```{r}
Gelman.Rubin <- function(psi) {
  psi <- as.matrix(psi)
  n <- ncol(psi)
  k <- nrow(psi)
  psi.means <- rowMeans(psi) 
  B <- n * var(psi.means) 
  psi.w <- apply(psi, 1, "var") 
  W <- mean(psi.w) 
  v.hat <- W*(n-1)/n + (B/n) 
  r.hat <- v.hat / W 
  return(r.hat)
}
normal.chain <- function(sigma, N, X1) {
  x <- rep(0, N)
  x[1] <- X1
  u <- runif(N)
  for (i in 2:N) {
    xt <- x[i-1]
    y <- rnorm(1, xt, sigma) 
    r1 <- (1/2*exp(-abs(y))) * dnorm(xt, y, sigma)
    r2 <- (1/2*exp(-abs(xt))) * dnorm(y, xt, sigma)
    r <- r1 / r2
    if (u[i] <= r) x[i] <- y else
      x[i] <- xt
  }
  return(x)
}
sigma <- 2 
k <- 4 
n <- 15000 
b <- 1000 
x0 <- c(-10, -5, 5, 10)
X <- matrix(0, nrow=k, ncol=n)
for (i in 1:k)
  X[i, ] <- normal.chain(sigma, n, x0[i])
psi <- t(apply(X, 1, cumsum))
for (i in 1:nrow(psi))
  psi[i,] <- psi[i,] / (1:ncol(psi))
print(Gelman.Rubin(psi))


for (i in 1:k)
  plot(psi[i, (b+1):n], type="l",
       xlab=i, ylab=bquote(psi))

rhat <- rep(0, n)
for (j in (b+1):n)
  rhat[j] <- Gelman.Rubin(psi[,1:j])
plot(rhat[(b+1):n], type="l", xlab="", ylab="R")
abline(h=1.2, lty=2)
```

## Question 3

Find the intersection points A(k) in $(0,\sqrt{k})$ of the curves $$S_{k-1}(a)=P\left(t(k-1)>\sqrt{\frac{a^2(k-1)}{k-a^2}}\right)$$

and$$S_{k}(a)=P\left(t(k)>\sqrt{\frac{a^2k}{k+1-a^2}}\right)$$

for k = 4 : 25, 100, 500, 1000, where t(k) is a Student t random variable with
k degrees of freedom. (These intersection points determine the critical values
for a t-test for scale-mixture errors proposed by Sz´ekely [260].)

## Answer 3

use bisection method to find the root for k = 4 : 25, 100, 500, 1000

```{r}
f <- function(a, k) {
  pt(sqrt(a^2*(k-1)/(k-a^2)),k-1)-pt(sqrt(a^2*k/(k+1-a^2)),k)
}
k <- c(4:25,100,500,1000)
A <- numeric(length(k))
for (i in 1:length(k)){
  b0 <- 0
  b1 <- sqrt(k[i])
  it <- 0
  eps <- .Machine$double.eps^0.25
  r <- seq(b0, b1-eps, length=3)
  y <- c(f(r[1], k[i]), f(r[2],k[i]), f(r[3],k[i]))
  if (y[1] * y[3] > 0)
    stop("f does not have opposite sign at endpoints")
  while(it < 1000 && abs(y[2]) > eps) {
    it <- it + 1
    if (y[1]*y[2] < 0) {
      r[3] <- r[2]
      y[3] <- y[2]
   } else {
      r[1] <- r[2]
      y[1] <- y[2]
   }
   r[2] <- (r[1] + r[3]) / 2
   y[2] <- f(r[2],k=k[i])
  }
  A[i] <- r[2]
}
print(A)
```

## Question 1

(1)Use EM algorithm to solve MLE of p and q (consider missing
data $n_{AA}$ and $n_{BB}$).

(2)Record the values of p and q that maximize the conditional
likelihood in each EM steps, calculate the corresponding
log-maximum likelihood values (for observed data), are they
increasing?

## Answer 1

```{r}
na <- 444
nb <- 132
noo <- 361
nab <- 63
n <- na + nb + noo +nab
k <- 10    ##迭代次数
p <- q <- r <- E <- numeric(k)
p[1] <- q[1] <- r[1] <- 1/3
for (i in 2:k){
  p.old <- p[i-1]
  q.old <- q[i-1]
  r.old <- r[i-1]
  Eaa <- na*p.old/(p.old+2*r.old)
  Ebb <- nb*q.old/(q.old+2*r.old)
  p[i] <- (Eaa+na+nab)/2/n
  q[i] <- (Ebb+nb+nab)/2/n
  r[i] <- 1-p[i]-q[i]
  E[i] <- na*p[i]/(p[i]+2*r[i])*log(p[i]/2/r[i])+
    nb*q[i]/(q[i]+2*r[i])*log(q[i]/2/r[i])+2*noo*log(r[i])+
    na*log(2*p[i]*r[i])+nb*log(2*q[i]*r[i])+nab*log(2*p[i]*q[i])
}
print(matrix(c(p,q,r,E),10,4))
```
从结果我们可以看到经过约8次迭代之后达到收敛，并且期望的值确实在增大。

## Question 2

Use both for loops and lapply() to fit linear models to the
mtcars using the formulas stored in this list:

formulas <- list(

mpg ~ disp,

mpg ~ I(1 / disp),

mpg ~ disp + wt,

mpg ~ I(1 / disp) + wt

)

## Answer 2

```{r}
formulas <- list(
  mpg ~ disp,
  mpg ~ I(1 / disp),
  mpg ~ disp + wt,
  mpg ~ I(1 / disp) + wt
)
a <- as.character(formulas)
out1 <- out2 <- vector("list", length(a))
options(warn = -1)
for (i in seq_along(formulas)){
  out1[i] <- lm(a[i] ,mtcars)
}
out2 <- lapply(formulas, function(x) lm(x,mtcars))
print(out1)
print(out2)
```

## Question 3

The following code simulates the performance of a t-test for
non-normal data. Use sapply() and an anonymous function
to extract the p-value from every trial.

trials <- replicate(

100,

t.test(rpois(10, 10), rpois(7, 10)),

simplify = FALSE

)

Extra challenge: get rid of the anonymous function by using
[[ directly.

## Answer 3

```{r}
trials <- replicate(
  100,
  t.test(rpois(10, 10), rpois(7, 10)),
  simplify = FALSE
)
p <- vector("list", length(trials))
p <- sapply(trials, function(x) x[["p.value"]] )
print(p)
```

## Question 4

Implement a combination of Map() and vapply() to create an
lapply() variant that iterates in parallel over all of its inputs
and stores its outputs in a vector (or a matrix). What arguments
should the function take?

## Answer 4

```{r}
mvapply <- function(x,f,f_value){
  vapply(x, function(x) Map(f,x), f_value)
}
```
需要的参数：数据x，函数f，函数f返回值的形式模板f_value

## Question 1

Write an Rcpp function for Exercise 9.4

## Answer 1

```{r}
library(Rcpp) 
cppFunction('NumericVector rw_Metropolis(double sigma,double x0,int N) {
  NumericVector x(N);
  x[0]=x0;
  NumericVector u(N);
  u=runif(N);
  for(int i=1;i<N;i++){
    NumericVector y(1);
    y=rnorm(1, x[i-1], sigma);
    if(u[i] <= ((exp(-abs(y[0]))) / (exp(-abs(x[i-1]))))){
      x[i] = y[0];
    }
    else{
      x[i] = x[i-1];
    }
  }
  return x;
}')
sigma <- c(.05, .5, 2, 16)
x0 <- 25
N <- 2000
rw1 = rw_Metropolis(sigma[1],x0,N)
rw2 = rw_Metropolis(sigma[2],x0,N)
rw3 = rw_Metropolis(sigma[3],x0,N)
rw4 = rw_Metropolis(sigma[4],x0,N)

rw = cbind(rw1, rw2, rw3,  rw4)
for (j in 1:4) {
  plot(rw[,j], type="l",
       xlab=bquote(sigma == .(round(sigma[j],3))),
       ylab="X", ylim=range(rw[,j]))
}
```

## Question 2

Compare the corresponding generated random numbers with
those by the R function you wrote before using the function
“qqplot”.

## Answer 2

the qqplot of C++ function
```{r}

for (j in 1:4) {
  qqplot(1:2000,rw[,j], type="l",
       xlab=bquote(sigma == .(round(sigma[j],3))),
       ylab="X", ylim=range(rw[,j]))
}
```

the qqplot of R function
```{r}
set.seed(3000)

lap_f = function(x) exp(-abs(x))

rw.Metropolis = function(sigma, x0, N){
 x = numeric(N)
 x[1] = x0
 u = runif(N)
 k = 0
 for (i in 2:N) {
  y = rnorm(1, x[i-1], sigma)
  if (u[i] <= (lap_f(y) / lap_f(x[i-1]))) x[i] = y 
  else {
  x[i] = x[i-1]
  k = k+1
  }
 }
 return(list(x = x, k = k))
}

N = 2000
sigma = c(.05, .5, 2, 16)
x0 = 25
rw1 = rw.Metropolis(sigma[1],x0,N)
rw2 = rw.Metropolis(sigma[2],x0,N)
rw3 = rw.Metropolis(sigma[3],x0,N)
rw4 = rw.Metropolis(sigma[4],x0,N)
 
    rw = cbind(rw1$x, rw2$x, rw3$x,  rw4$x)
    for (j in 1:4) {
        qqplot(1:2000,rw[,j], type="l",
             xlab=bquote(sigma == .(round(sigma[j],3))),
             ylab="X", ylim=range(rw[,j]))
    }
```

## Question 3

Campare the computation time of the two functions with the
function “microbenchmark”.

## Answer 3

```{r}
library(microbenchmark)
sigma0 <- 2
ts <- microbenchmark(
  rw_Metropolis(sigma0, x0, N),
  rw.Metropolis(sigma0, x0, N)
)
summary(ts)[,c(1,3,5,6)]
```

## Question 4

Comments your results.

## Answer 4

the results of C++ function and R function are the same, but C++ function is much faster.